{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "def sigmoid(x):\n    return 1/(1+np.exp(-x))\n\ndef sigmoid_der(x):\n    return (x)*(1-(x))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "source": "# def neural_a(trainFile, paramFile, weightFile):\ntrainFile \u003d \"train.csv\"\nparamFile \u003d \"param.txt\"",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": "\nasArrays \u003d pd.read_csv(trainFile,header\u003dNone).values\nX \u003d asArrays[:,:(asArrays.shape[1] - 1)]\ny \u003d asArrays[:,(asArrays.shape[1]-1)]\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": "\nparams \u003d open(paramFile,\u0027r\u0027)\nlearningRateType \u003d int(params.readline(),10)\nlearningRate \u003d float(params.readline())\nmaxIter \u003d int(params.readline(),10)\nbatchSize \u003d int(params.readline(),10)\nnnArchitecture \u003d params.readline().rstrip().split(\u0027 \u0027)\nnnArchitecture \u003d list(map(int, nnArchitecture))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "source": "wdict \u003d {}\nzdict \u003d {}\nadict \u003d {}\n# +1 done everywhere to include bias term in the weights matrix itself\nwdict[0] \u003d np.zeros((X.shape[1] + 1,nnArchitecture[0]))\n\ntotalLayers \u003d len(nnArchitecture)\nn \u003d X.shape[0]\n\nfor i in range(len(nnArchitecture)):\n    if i!\u003d0:\n        wdict[i] \u003d np.zeros((nnArchitecture[i-1] + 1,nnArchitecture[i]))\n\nwdict[totalLayers] \u003d np.zeros((nnArchitecture[totalLayers-1] + 1,1))\ndw \u003d wdict\n# print(len(wdict))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "(50, 51)\n(50, 51)\n"
          ],
          "output_type": "stream"
        },
        {
          "evalue": "shapes (50,52) and (51,1) not aligned: 52 (dim 1) !\u003d 51 (dim 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-29-99e2a19c8680\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0madict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 25\u001b[0;31m     \u001b[0mzdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotalLayers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotalLayers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotalLayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0madict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotalLayers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotalLayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# print(iterations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (50,52) and (51,1) not aligned: 52 (dim 1) !\u003d 51 (dim 0)"
          ],
          "ename": "ValueError",
          "output_type": "error"
        }
      ],
      "source": "\niterations \u003d 0\nwhile iterations\u003cmaxIter:\n    iterations+\u003d1\n    \n    k \u003d ((iterations-1)*batchSize)%n\n    workingX \u003d X[k:(k+batchSize)]\n    workingy \u003d y[k:(k+batchSize)]\n    \n    if learningRateType\u003d\u003d1:\n        rate \u003d learningRate\n    else:\n        rate \u003d learningRate/np.sqrt(iterations)\n    #forward propogation\n    \n    adict[-1] \u003d np.hstack((np.ones((workingX.shape[0],1)),workingX))\n    for i in range(totalLayers):\n        zdict[i] \u003d np.dot(adict[i-1],wdict[i])\n        print(zdict[i].shape)\n\n        temp \u003d sigmoid(zdict[i])\n        print(temp.shape)\n\n        adict[i] \u003d np.hstack((np.ones((temp.shape[0],1)),temp))\n    zdict[totalLayers] \u003d np.dot(adict[totalLayers-1],wdict[totalLayers])\n    adict[totalLayers] \u003d sigmoid(zdict[totalLayers])\n    # print(iterations)\n    # print(\"adict[-1]\")\n    # print(adict[-1])\n    # print(\"adict[0]\")\n    # print(adict[0])\n    # print(\"adict[1]\")\n    # print(adict[1])\n    \n    #backward propogation\n    \n    #error of last layer\n    error \u003d adict[totalLayers] - workingy.reshape([workingy.size,1])\n    # print(\"error size\" + str(error.shape))\n    \n    #dw for last layer\n    dw[totalLayers] \u003d np.dot(adict[totalLayers-1].T,error)/batchSize\n    print(\"dw[totalLayers] size\" + str(dw[totalLayers].shape))\n    # print(wdict[totalLayers].shape)\n    \n    #da \u003d error.weights/batchsize\n    da \u003d np.dot(error,wdict[totalLayers].T)/batchSize\n    print(\"da[totalLayers] size: \" + str(da.shape))\n    \n    #dz \u003d da*sigmoid_der(a values)\n    dz \u003d da*sigmoid_der(adict[totalLayers])\n    # dz\n    dz[:,0] \u003d np.zeros(dz[:,0].shape)\n    # dz \u003d dz[:,1:]\n    \n    print(\"dz size: \"+str(dz.shape))\n\n    for i in reversed(range(totalLayers)):\n        #general dw\n        dw[i] \u003d np.dot(adict[i-1].T,dz)\n        \n        print(\"adict[\" + str(i)+\"] size: \"+str(adict[i].shape))\n        print(\"dz size: \"+str(dz.shape))\n\n        \n        \n        \n        print(\"wdict[\" + str(i)+\"] size: \"+str(wdict[i].shape))\n        print(\"dw[\" + str(i)+\"] size: \"+str(dw[i].shape))\n        \n        #updating da\n        da \u003d np.dot(dz,wdict[i+1])\n        print(\"da[\" + str(i)+\"] size: \"+str(da[i].shape))\n        \n        #updating dz\n        dz \u003d da*sigmoid_der(adict[i])\n        dz[:,0] \u003d np.zeros(dz[:,0].shape)\n\n        #removing one layer from dz, which would have updated the bias term\n        # dz[:,0] \u003d np.zeros(dz.shape(1))\n        print(\"dz size: \"+str(dz.shape))\n        # print(dz)\n\n        dw[i] \u003d np.dot(adict[i-1].T,dz)\n        print(\"dw[\" + str(i)+\"] size: \"+str(dw[i].shape))\n        print(\"wdict[\" + str(i)+\"] size: \"+str(wdict[i].shape))\n\n        wdict[i] -\u003d rate*dw[i]\n        \n\n       \n        \n    \n    \n    ",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}